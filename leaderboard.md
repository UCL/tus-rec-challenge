---
title: Leaderboard
layout: default
nav_order: 10
---
| **Rank** | **Team Name** | **Team Members**                              | **Team Affiliation**                      | **Final Score** | **Global Score** | **Local Score** | **Pixel Score** | **Landmark Score** |     **GPE (mm)**      |     **GLE (mm)**      |     **LPE (mm)**     |     **LLE (mm)**     | **Run Time (s)** |
|----------|---------------|-----------------------------------------------|-------------------------------------------|-----------------|------------------|-----------------|-----------------|--------------------|------------------|------------------|-----------------|-----------------|------------------|
|  1   |             Baseline             | TUS-REC Organisation Team | University College London, United Kingdom | **0.163±0.216** | -0.078±0.410 | 0.405±0.051 | 0.244±0.145 |  0.083±0.304   | 26.110±7.256 | 23.681±9.049 | 0.214±0.025 | 0.181±0.030 | 19.753±1.570 |


> Note: All scores (the larger the better) are normalised using the ranking method described in the <a href="https://github-pages.ucl.ac.uk/tus-rec-challenge/assessment.html#ranking-method" target="_blank">assessment page</a>. The raw values of DDF errors (the smaller the better) before normalisation are also listed for your reference. All values are rounded to 3 decimal places. 